（2024-08-20：注意repo更新了）
以下是本地部署LLM（eg千问）和Graphrag的避坑指南。因为本地部署比用openai的麻烦一些，所以需要使用者已经熟悉LLM生态和常用操作。

克隆Graphrag。【注意】因为a）和中文相关的问题是近期才集中爆出来的，Graphrag对中文的支持并不好。此外b）对于开源大模型的支持也不行。对于中文，可以看下Graphrag repo里面的issue #596，对于开源大模型可以看#609。考虑到Graphrag随时更新，pip库可能会有更新延迟，万能药是从github安装，pip install git+ graphrag仓库地址

ollama起LLM，LM Studio起embedding模型，或者hack下openai_embeddings_llm.py (eg路径 venv/lib/python3.11/site-packages/graphrag/llm/openai/openai_embeddings_llm.py

首先初始化python -m graphrag.index --init --root .

如果是ollama起LLM，LM Studio起embedding，修改yaml文件的llm和embedding部分
llm model改成your model
llm api_base改成 http://localhost:11434/v1 即ollama的端口
llm request_timeout建议从180改大，我用的3600。避免后续接到timeout error （如果你在log中持续看到）
embedding model改成你的model
embedding api_base改成http://localhost:1234/v1 即LM Studio的端口

如果是hack openai_embeddings_llm.py，修改36-40行（参见介绍最后部分）

因为对中文支持不好，用默认的prompt template大概率也是有问题的。详情可以看https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/ 这里建议prompt tuning command是 python -m graphrag.prompt_tune -- root . --language Chinese --output prompts_zh --no-entity-types。【注意】一定要加no entity types，此外避免加别的参数，我跑的时候测了其他的都会报错。这样出来的template是能用，但是更好的效果，需要手动修改。也有人把英文template扔给chatgpt翻译，去掉不必要的例子等等。但是手动修改，确保中文输出格式和英文中template格式是一致的。

然后在yaml文件里修改GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE = "prompts_zh/entity_extraction.txt"；GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE = "prompts_zh/community_report.txt"；GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE = "prompts_zh/summarize_descriptions.txt"；即从默认的prompts改为刚才指定的prompts_zh

之后就可以正常的建立index和问询测试。

hack openai_embeddings_llm.py （不建议，万一后面它又改了）
BEFORE：
embedding = await self.client.embeddings.create(
      input=input,
      **args,
    )
return [d.embedding for d in embedding.data]

AFTER：
embedding_list = []
for inp in input:
embedding = ollama.embeddings(model=你的model, prompt = inp)
embedding_list.append(embedding["embedding"])
return embedding_list